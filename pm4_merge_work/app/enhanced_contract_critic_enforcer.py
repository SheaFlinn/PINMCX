#!/usr/bin/env python3
"""
Enhanced Contract Critic & Market Balance Enforcer - World-Class 50/50 Enforcement

Implements strict blocking enforcement for Memphis civic prediction market:
- All market balance issues are 1.0 weight (blocking)
- Explicit LLM-driven checks: "Would both sides bet this?", "Is this genuinely uncertain?"
- Multi-variant reframing integration
- Complete admin override logging and audit trail
- No drift, no weak spots - only genuinely bettable contracts pass

Version: July_25_2025v2_WorldClass - Complete Market Viability Integration
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from openai import OpenAI

logger = logging.getLogger(__name__)

@dataclass
class CriticAnalysis:
    """Results from enhanced contract critic analysis"""
    overall_score: float  # 0.0 to 1.0
    passed: bool
    blocked: bool  # True if contract is blocked from publication
    issues_found: List[Dict[str, Any]]
    blocking_issues: List[Dict[str, Any]]  # Issues that block publication
    rewrite_suggestions: List[str]
    rejection_reason: Optional[str]
    critic_notes: str
    analysis_timestamp: datetime
    market_balance_score: float  # 0.0 to 1.0 - specific to 50/50 balance
    admin_override_required: bool  # True if admin override needed for publication
    rubric_version: str  # Version of rubric used for analysis
    adversarial_test_result: str  # Result of insider knowledge test
    bookie_viability_assessment: str  # Would real bookies list this?
    variant_analysis: Optional[Dict]  # Analysis of multiple variants

@dataclass
class VariantAnalysis:
    """Analysis of multiple contract variants"""
    total_variants: int
    variants_passed: int
    variants_blocked: int
    best_variant: Optional[Dict]
    variant_results: List[Dict]
    recommendation: str  # "publish_best", "admin_rescue", "reject_all"

class EnhancedContractCriticEnforcer:
    """
    Enhanced LLM-powered contract critic with world-class 50/50 enforcement
    No drift, no weak spots - only genuinely bettable contracts pass
    """
    
    def __init__(self):
        """Initialize the enhanced contract critic enforcer"""
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.model = "gpt-4"
        
        # Initialize logger for institutional-grade audit trail
        self.logger = logging.getLogger(__name__)
        
        # PRODUCTION THRESHOLDS - July 28, 2025 Market Viability Upgrade (Tuned for Civic Markets)
        self.min_passing_score = 0.2  # Further lowered to reduce over-blocking
        self.market_balance_threshold = 0.4  # Relaxed for realistic civic events
        self.critical_issue_threshold = 0.9
        
        # PROBABILITY BIAS THRESHOLDS - Realistic Civic Market Range
        self.min_viable_probability = 0.10  # Only block truly impossible events (<10%)
        self.max_viable_probability = 0.90  # Only block near-certain events (>90%)
        
        # Response caching for development/testing speed
        self.response_cache = {}
        self.enable_caching = os.getenv('ENABLE_CRITIC_CACHING', 'false').lower() == 'true'
        
        # Deterministic settings
        self.temperature = 0.0  # Maximum determinism
        self.seed = 42  # Fixed seed for reproducibility
        
        # Issue type weights - ALL MARKET BALANCE ISSUES ARE 1.0 (BLOCKING)
        self.issue_weights = {
            # BLOCKING ISSUES (1.0 weight) - No publication without admin override
            'biased_framing': 1.0,      # Political bias or loaded language
            'probability_bias': 1.0,    # Overwhelmingly likely/unlikely (>80%/<20%)
            'market_viability': 1.0,    # Rational bettors won't take both sides
            'trading_balance': 1.0,     # Real bookies wouldn't list this market
            'unbettable': 1.0,          # Technical betting impossibility
            
            # QUALITY ISSUES (weighted but not blocking)
            'ambiguity': 0.9,           # Unclear resolution criteria
            'pseudo_ostension': 0.8,    # Fake drama/manufactured controversy
            'arc_fatigue': 0.6,         # Story getting stale/repetitive
            'unclear_resolution': 0.9   # Resolution disputes likely
        }
        
        # Blocking issue types - PRODUCTION CALIBRATION (July 28, 2025)
        # Only issues with 1.0 weight block contracts
        self.blocking_issue_types = {
            'biased_framing', 'probability_bias', 'market_viability', 'trading_balance', 'unbettable'
        }
        
        # Rubric version for audit trail
        self.rubric_version = "July_25_2025v2_WorldClass_Enforcer"
    
    def analyze_contract_with_variants(self, contract: Dict[str, Any], 
                                     variants: List[Dict[str, Any]] = None,
                                     arc_context: Optional[Dict] = None,
                                     recent_contracts: Optional[List[Dict]] = None) -> CriticAnalysis:
        """
        Analyze contract and all variants with strict 50/50 enforcement
        
        Args:
            contract: Primary contract to analyze
            variants: List of contract variants (minimum 3 required)
            arc_context: Related arc/narrative context
            recent_contracts: Recent contracts for fatigue detection
            
        Returns:
            CriticAnalysis with detailed findings for all variants
        """
        
        # If no variants provided, analyze just the primary contract
        if not variants:
            return self.analyze_single_contract(contract, arc_context, recent_contracts)
        
        # Analyze all variants
        variant_results = []
        best_variant = None
        best_score = 0.0
        variants_passed = 0
        variants_blocked = 0
        
        # Analyze primary contract
        primary_analysis = self.analyze_single_contract(contract, arc_context, recent_contracts)
        variant_results.append({
            'variant_type': 'primary',
            'contract': contract,
            'analysis': primary_analysis,
            'passed': primary_analysis.passed,
            'blocked': primary_analysis.blocked,
            'market_balance_score': primary_analysis.market_balance_score
        })
        
        if primary_analysis.passed and not primary_analysis.blocked:
            variants_passed += 1
            if primary_analysis.market_balance_score > best_score:
                best_variant = contract
                best_score = primary_analysis.market_balance_score
        elif primary_analysis.blocked:
            variants_blocked += 1
        
        # Analyze all provided variants
        for i, variant in enumerate(variants):
            variant_analysis = self.analyze_single_contract(variant, arc_context, recent_contracts)
            variant_results.append({
                'variant_type': f'variant_{i+1}',
                'contract': variant,
                'analysis': variant_analysis,
                'passed': variant_analysis.passed,
                'blocked': variant_analysis.blocked,
                'market_balance_score': variant_analysis.market_balance_score
            })
            
            if variant_analysis.passed and not variant_analysis.blocked:
                variants_passed += 1
                if variant_analysis.market_balance_score > best_score:
                    best_variant = variant
                    best_score = variant_analysis.market_balance_score
            elif variant_analysis.blocked:
                variants_blocked += 1
        
        # Determine recommendation
        total_variants = len(variant_results)
        if variants_passed > 0:
            recommendation = "publish_best"
        elif variants_blocked == total_variants:
            recommendation = "reject_all"
        else:
            recommendation = "admin_rescue"
        
        # Create variant analysis summary
        variant_analysis_summary = VariantAnalysis(
            total_variants=total_variants,
            variants_passed=variants_passed,
            variants_blocked=variants_blocked,
            best_variant=best_variant,
            variant_results=variant_results,
            recommendation=recommendation
        )
        
        # Use the best result as the primary analysis, or primary if none passed
        if best_variant:
            best_result = next(r for r in variant_results if r['contract'] == best_variant)
            primary_result = best_result['analysis']
        else:
            primary_result = primary_analysis
        
        # Enhance the primary result with variant analysis
        primary_result.variant_analysis = asdict(variant_analysis_summary)
        
        return primary_result
    
    def analyze_single_contract(self, contract: Dict[str, Any], 
                              arc_context: Optional[Dict] = None,
                              recent_contracts: Optional[List[Dict]] = None) -> CriticAnalysis:
        """
        Perform comprehensive LLM-based analysis of single contract with strict enforcement
        
        Args:
            contract: Contract to analyze
            arc_context: Related arc/narrative context
            recent_contracts: Recent contracts for fatigue detection
            
        Returns:
            CriticAnalysis with detailed findings and strict blocking enforcement
        """
        
        try:
            # Build analysis prompt with enhanced market balance questions
            analysis_prompt = self._build_enhanced_analysis_prompt(contract, arc_context, recent_contracts)
            
            # Get LLM analysis
            llm_response = self._query_llm_critic(analysis_prompt)
            
            # Parse LLM response
            critic_result = self._parse_llm_response(llm_response)
            
            # Calculate scores
            overall_score = self._calculate_overall_score(critic_result['issues'])
            market_balance_score = self._calculate_market_balance_score(critic_result['issues'])
            
            # Identify blocking issues
            blocking_issues = self._identify_blocking_issues(critic_result['issues'])
            
            # Determine if contract is blocked (any blocking issue = blocked)
            blocked = len(blocking_issues) > 0
            
            # Determine pass/fail - ONLY BLOCK ON MARKET VIABILITY METRICS (Emergency Fix July 27, 2025v3)
            # Overall score is for analytics only - never blocks contracts
            market_viability_passed = (market_balance_score >= self.market_balance_threshold)
            passed = (market_viability_passed and not blocked)
            # Removed critical issues check - only blocking issues should block contracts
            
            # Check if admin override required
            admin_override_required = blocked or self._requires_admin_override(critic_result['issues'])
            
            # Generate rewrite suggestions and rejection reason
            rewrite_suggestions = []
            rejection_reason = None
            
            if not passed or blocked:
                rewrite_suggestions = self._generate_enhanced_rewrite_suggestions(contract, critic_result['issues'])
                rejection_reason = self._generate_enhanced_rejection_reason(critic_result['issues'], blocked)
            
            return CriticAnalysis(
                overall_score=overall_score,
                passed=passed,
                blocked=blocked,
                issues_found=critic_result['issues'],
                blocking_issues=blocking_issues,
                rewrite_suggestions=rewrite_suggestions,
                rejection_reason=rejection_reason,
                critic_notes=critic_result.get('notes', ''),
                analysis_timestamp=datetime.utcnow(),
                market_balance_score=market_balance_score,
                admin_override_required=admin_override_required,
                rubric_version=self.rubric_version,
                adversarial_test_result=critic_result.get('adversarial_test_result', 'Unknown'),
                bookie_viability_assessment=critic_result.get('bookie_viability', 'Unknown'),
                variant_analysis=None  # Set by variant analysis if applicable
            )
            
        except Exception as e:
            logger.error(f"Error in enhanced contract critic analysis: {str(e)}")
            # Return conservative failure on error
            return CriticAnalysis(
                overall_score=0.0,
                passed=False,
                blocked=True,  # Block on error for safety
                issues_found=[{
                    'issue_type': 'analysis_error',
                    'severity': 'critical',
                    'description': f'Critic analysis failed: {str(e)}',
                    'confidence': 1.0,
                    'market_balance_impact': 'high'
                }],
                blocking_issues=[{
                    'issue_type': 'analysis_error',
                    'severity': 'critical',
                    'description': f'Critic analysis failed: {str(e)}',
                    'confidence': 1.0
                }],
                rewrite_suggestions=[],
                rejection_reason='Technical analysis failure - contract blocked for safety',
                critic_notes='Enhanced critic encountered an error during analysis',
                analysis_timestamp=datetime.utcnow(),
                market_balance_score=0.0,
                admin_override_required=True,
                rubric_version=self.rubric_version,
                adversarial_test_result='Error',
                bookie_viability_assessment='Error',
                variant_analysis=None
            )
    
    def _build_enhanced_analysis_prompt(self, contract: Dict[str, Any], 
                                      arc_context: Optional[Dict] = None,
                                      recent_contracts: Optional[List[Dict]] = None) -> str:
        """Build enhanced analysis prompt with strict 50/50 enforcement questions"""
        
        prompt = f"""You are an expert adversarial contract critic for a civic prediction market with INSTITUTIONAL-GRADE market viability enforcement. Your job is to ensure ONLY genuinely marketable outcomes that would attract rational bettors are published.

CONTRACT TO ANALYZE:
Title: {contract.get('title', 'N/A')}
Description: {contract.get('description', 'N/A')}
Actor: {contract.get('actor', 'N/A')}
Timeline: {contract.get('timeline', 'N/A')}
Resolution Criteria: {contract.get('resolution_criteria', 'N/A')}
Source: {contract.get('source', 'N/A')}

MARKET VIABILITY ASSESSMENT - PRODUCTION STANDARDS:

**PRIMARY FOCUS: Only block contracts that are clearly outside 30-70% probability range or technically unresolvable.**

1. PROBABILITY BIAS CHECK (30-70% VIABLE RANGE):
   - PASS: Any civic event with genuine uncertainty (30-70% probability range)
   - BLOCK ONLY IF: >90% certain (e.g., "Will the sun rise?") or <10% likely (impossible events)
   - Memphis civic events (budget votes, elections, policy decisions) are typically VIABLE
   - Normal political uncertainty and debate = MARKETABLE, not problematic
   - Consider: Would reasonable people disagree about the likelihood?

2. MARKET VIABILITY TEST (RELAXED FOR CIVIC EVENTS):
   - PASS: Standard civic/political events with clear resolution criteria
   - Memphis council votes, elections, policy implementations = VIABLE by default
   - Public information availability is normal for civic events
   - "Insider knowledge" concerns are overblown for public civic processes
   - BLOCK ONLY: Truly private/confidential decisions with no public information

3. CIVIC RELEVANCE CHECK:
   - PASS: Any genuine Memphis civic/political/community event
   - Elections, budget votes, policy decisions, public projects = VIABLE
   - BLOCK ONLY: Obvious nonsense ("Will aliens land?"), purely hypothetical scenarios
   - Real civic events with real consequences = MARKETABLE

4. RESOLUTION CLARITY:
   - PASS: Clear, objective resolution criteria with reliable sources
   - Official votes, election results, policy announcements = CLEAR
   - BLOCK ONLY: Subjective outcomes ("Will people be happier?"), unverifiable claims
   - Standard civic documentation and official announcements = SUFFICIENT"""

        if arc_context:
            prompt += f"""

ARC CONTEXT:
Arc ID: {arc_context.get('arc_id', 'N/A')}
Arc Name: {arc_context.get('arc_name', 'N/A')}
Arc Type: {arc_context.get('arc_type', 'N/A')}
Contracts in Arc: {arc_context.get('contract_count', 0)}"""

        if recent_contracts:
            prompt += f"""

RECENT SIMILAR CONTRACTS (for fatigue detection):
{json.dumps([{k: v for k, v in c.items() if k in ['title', 'description', 'arc_id']} for c in recent_contracts[:5]], indent=2)}"""

        prompt += """

ANALYSIS FRAMEWORK - July_25_2025v2 World-Class Enforcement:

**BLOCKING ISSUES (1.0 WEIGHT) - PREVENT PUBLICATION:**

1. PROBABILITY BIAS (BLOCKING):
   - Is this outcome >80% likely or <20% likely based on current facts?
   - Would rational bettors heavily favor one side?
   - Does public information make one outcome near-certain?

2. MARKET VIABILITY (BLOCKING):
   - Would rational bettors take BOTH sides of this bet?
   - Is there genuine uncertainty attracting balanced action?
   - Would this generate real trading interest on both YES and NO?

3. TRADING BALANCE (BLOCKING):
   - Would professional bookmakers list this market?
   - Is this a "dead" market with predetermined outcome?
   - Would traders consider this viable for balanced betting?

4. BIASED FRAMING (BLOCKING):
   - Is the contract framed neutrally without political bias?
   - Does wording artificially skew odds or push agenda?
   - Are terms loaded or leading toward one outcome?

5. UNBETTABLE (BLOCKING):
   - Can this be fairly bet on with clear resolution?
   - Is the outcome genuinely uncertain (not predetermined)?
   - Will resolution be unambiguous?

**QUALITY ISSUES (WEIGHTED BUT NOT BLOCKING):**

6. AMBIGUITY: Clear decision maker, action, timeline, resolution criteria
7. PSEUDO-OSTENSION: Genuine civic drama vs manufactured controversy
8. ARC FATIGUE: Story freshness and narrative value
9. UNCLEAR RESOLUTION: Resolution clarity and dispute potential

RESPONSE FORMAT (JSON):
{
  "issues": [
    {
      "issue_type": "probability_bias|market_viability|trading_balance|biased_framing|unbettable|ambiguity|pseudo_ostension|arc_fatigue|unclear_resolution",
      "severity": "low|medium|high|critical",
      "description": "Specific description of the issue",
      "suggested_fix": "How to fix this issue (optional)",
      "confidence": 0.0-1.0,
      "market_balance_impact": "high|medium|low",
      "blocking": true/false
    }
  ],
  "overall_assessment": "Brief overall assessment",
  "market_balance_assessment": "Assessment of 50/50 balance viability",
  "adversarial_test_result": "Result of insider knowledge test - would insiders avoid one side?",
  "bookie_viability": "Would real bookies list this market? Why/why not?",
  "rational_bettor_analysis": "Would rational bettors take both sides?",
  "notes": "Additional critic notes"
}

Be EXTREMELY rigorous. Any contract failing the 4 critical tests above should be flagged with blocking issues. Only genuinely uncertain, balanced, bettable contracts should pass."""

        return prompt
    
    def _identify_blocking_issues(self, issues: List[Dict]) -> List[Dict]:
        """Identify issues that block contract publication - EMERGENCY FIX July 27, 2025v3"""
        blocking_issues = []
        for issue in issues:
            # ONLY use blocking_issue_types set - ignore LLM blocking flags
            if issue.get('issue_type') in self.blocking_issue_types:
                blocking_issues.append(issue)
        return blocking_issues
    
    def _calculate_market_balance_score(self, issues: List[Dict]) -> float:
        """Calculate market balance specific score (0.0 to 1.0)"""
        market_balance_issues = ['probability_bias', 'market_viability', 'trading_balance', 'biased_framing']
        
        total_impact = 0.0
        max_impact = len(market_balance_issues)
        
        for issue_type in market_balance_issues:
            issue_found = any(issue['issue_type'] == issue_type for issue in issues)
            if issue_found:
                # Find the issue and get its impact
                issue = next((i for i in issues if i['issue_type'] == issue_type), None)
                if issue:
                    impact_map = {'high': 1.0, 'medium': 0.6, 'low': 0.3}
                    impact = impact_map.get(issue.get('market_balance_impact', 'medium'), 0.6)
                    total_impact += impact
        
        # Return score (higher is better)
        return max(0.0, 1.0 - (total_impact / max_impact))
    
    def _generate_enhanced_rejection_reason(self, issues: List[Dict], blocked: bool) -> str:
        """Generate enhanced rejection reason with blocking context"""
        if blocked:
            blocking_issues = self._identify_blocking_issues(issues)
            blocking_types = [issue['issue_type'] for issue in blocking_issues]
            
            reason = f"Contract BLOCKED due to market balance failures: {', '.join(blocking_types)}. "
            
            if 'probability_bias' in blocking_types:
                reason += "Outcome is overwhelmingly likely/unlikely (>80%/<20%). "
            if 'market_viability' in blocking_types:
                reason += "Rational bettors would not take both sides. "
            if 'trading_balance' in blocking_types:
                reason += "Real bookies would not list this market. "
            if 'biased_framing' in blocking_types:
                reason += "Contract contains political bias or loaded language. "
            
            reason += "Admin override required for publication."
            return reason
        
        # Non-blocking rejection
        high_severity_issues = [i for i in issues if i.get('severity') in ['high', 'critical']]
        if high_severity_issues:
            issue_types = [i['issue_type'] for i in high_severity_issues]
            return f"Contract failed quality standards due to: {', '.join(issue_types)}"
        
        return "Contract failed to meet minimum quality thresholds"
    
    def _generate_enhanced_rewrite_suggestions(self, contract: Dict, issues: List[Dict]) -> List[str]:
        """Generate enhanced rewrite suggestions focusing on market balance"""
        suggestions = []
        
        blocking_issues = self._identify_blocking_issues(issues)
        
        for issue in blocking_issues:
            issue_type = issue['issue_type']
            
            if issue_type == 'probability_bias':
                suggestions.append("Reframe to create genuine uncertainty - add timing constraints, margin requirements, or process obstacles")
                suggestions.append("Consider alternative outcomes that would balance the probability")
                
            elif issue_type == 'market_viability':
                suggestions.append("Restructure to create compelling cases for both YES and NO outcomes")
                suggestions.append("Add uncertainty elements that would attract rational bettors to both sides")
                
            elif issue_type == 'trading_balance':
                suggestions.append("Modify scope or timeline to create a more tradeable market")
                suggestions.append("Focus on specific, uncertain aspects rather than predetermined outcomes")
                
            elif issue_type == 'biased_framing':
                suggestions.append("Remove loaded language and political bias from contract wording")
                suggestions.append("Reframe neutrally to avoid pushing particular agenda or outcome")
        
        # Add general suggestions for non-blocking issues
        for issue in issues:
            if issue not in blocking_issues and issue.get('suggested_fix'):
                suggestions.append(issue['suggested_fix'])
        
        return suggestions[:5]  # Limit to top 5 suggestions
    
    def _query_llm_critic(self, prompt: str) -> str:
        """Query LLM for enhanced contract criticism with institutional-grade consistency"""
        
        # Check cache first for development/testing speed
        cache_key = f"{hash(prompt)}_{self.model}_{self.temperature}_{self.seed}"
        if self.enable_caching and cache_key in self.response_cache:
            self.logger.info(f"âœ… CACHE HIT: Using cached response for prompt hash {hash(prompt)}")
            return self.response_cache[cache_key]
        
        try:
            # INSTITUTIONAL GRADE: Multi-run consistency check with majority rules
            consistency_runs = 3  # Require 3 runs for determinism
            results = []
            errors = []
            
            for run_num in range(consistency_runs):
                try:
                    # Deterministic OpenAI call with fixed seed
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {
                                "role": "system",
                                "content": "You are an expert contract critic for prediction markets with institutional-grade market viability standards. Analyze contracts for genuine market viability (30-70% probability range). Respond only in valid JSON format."
                            },
                            {
                                "role": "user", 
                                "content": prompt
                            }
                        ],
                        temperature=self.temperature,  # Deterministic
                        max_tokens=2000,
                        seed=self.seed  # Fixed seed for reproducibility
                    )
                    
                    result = response.choices[0].message.content.strip()
                    results.append(result)
                    
                    # Log every run for audit trail
                    self.logger.info(f"Critic Run {run_num + 1}/{consistency_runs}: {len(result)} chars")
                    
                except Exception as e:
                    error_msg = f"Critic Run {run_num + 1} failed: {str(e)}"
                    self.logger.error(error_msg)
                    errors.append(error_msg)
                    # Continue with other runs instead of failing completely
                    continue
            
            # Handle results with robust error checking
            if not results:
                error_summary = "; ".join(errors)
                raise Exception(f"All critic runs failed: {error_summary}")
            
            # Check for consistency across successful runs
            unique_results = list(set(results))
            if len(unique_results) == 1:
                # All runs identical - institutional grade consistency achieved
                self.logger.info(f"âœ… INSTITUTIONAL CONSISTENCY: All {len(results)} critic runs identical")
                final_result = results[0]
            else:
                # Split decision - use majority rules or first result
                self.logger.warning(f"âš ï¸ SPLIT DECISION: {len(unique_results)} different outcomes across {len(results)} runs")
                self.logger.warning(f"Results lengths: {[len(r) for r in results]}")
                
                # Log all results for admin review
                for i, result in enumerate(results):
                    self.logger.warning(f"Run {i+1} result: {result[:200]}...")
                
                # Use first result but flag for admin review
                final_result = results[0]
            
            # Cache the result for future use
            if self.enable_caching:
                self.response_cache[cache_key] = final_result
                self.logger.info(f"ðŸ’¾ CACHED: Response cached for future use")
            
            return final_result
                
        except Exception as e:
            error_msg = f"Error querying enhanced LLM critic: {str(e)}"
            self.logger.error(error_msg)
            # Return error response in expected JSON format
            return json.dumps({
                "issues": [{
                    "issue_type": "analysis_error",
                    "severity": "critical",
                    "description": f"LLM analysis failed: {str(e)}",
                    "confidence": 1.0,
                    "market_balance_impact": "high",
                    "blocking": True
                }],
                "overall_assessment": "Analysis failed",
                "market_balance_assessment": "Cannot assess due to error",
                "adversarial_test_result": "Error",
                "bookie_viability": "Error",
                "notes": "LLM query failed"
            })
    
    def _parse_llm_response(self, response: str) -> Dict:
        """Parse LLM response with enhanced error handling"""
        try:
            # Clean response and parse JSON
            cleaned_response = response.strip()
            if cleaned_response.startswith('```json'):
                cleaned_response = cleaned_response[7:]
            if cleaned_response.endswith('```'):
                cleaned_response = cleaned_response[:-3]
            
            parsed = json.loads(cleaned_response)
            
            # Validate required fields
            if 'issues' not in parsed:
                parsed['issues'] = []
            
            return parsed
            
        except json.JSONDecodeError as e:
            logger.error(f"Error parsing LLM response: {str(e)}")
            logger.error(f"Raw response: {response}")
            
            # Return error structure
            return {
                "issues": [{
                    "issue_type": "parse_error",
                    "severity": "critical",
                    "description": f"Failed to parse LLM response: {str(e)}",
                    "confidence": 1.0,
                    "market_balance_impact": "high",
                    "blocking": True
                }],
                "overall_assessment": "Parse error",
                "market_balance_assessment": "Cannot assess",
                "adversarial_test_result": "Error",
                "bookie_viability": "Error",
                "notes": "Response parsing failed"
            }
    
    def _calculate_overall_score(self, issues: List[Dict]) -> float:
        """Calculate overall quality score with enhanced weighting"""
        if not issues:
            return 1.0
        
        total_weight = 0.0
        weighted_score = 0.0
        
        for issue in issues:
            issue_type = issue.get('issue_type', 'unknown')
            severity = issue.get('severity', 'medium')
            confidence = issue.get('confidence', 1.0)
            
            # Get issue weight
            weight = self.issue_weights.get(issue_type, 0.5)
            
            # Calculate severity impact
            severity_impact = {
                'low': 0.2,
                'medium': 0.5,
                'high': 0.8,
                'critical': 1.0
            }.get(severity, 0.5)
            
            # Apply confidence weighting
            impact = severity_impact * confidence * weight
            
            total_weight += weight
            weighted_score += impact
        
        if total_weight == 0:
            return 1.0
        
        # Return score (higher is better)
        return max(0.0, 1.0 - (weighted_score / total_weight))
    
    def _has_critical_issues(self, issues: List[Dict]) -> bool:
        """Check for critical issues that require special handling"""
        for issue in issues:
            if issue.get('severity') == 'critical':
                return True
            if issue.get('confidence', 0) >= self.critical_issue_threshold:
                return True
        return False
    
    def _requires_admin_override(self, issues: List[Dict]) -> bool:
        """Determine if admin override is required"""
        # Any blocking issue requires admin override
        blocking_issues = self._identify_blocking_issues(issues)
        if blocking_issues:
            return True
        
        # Critical issues require override
        if self._has_critical_issues(issues):
            return True
        
        return False

# Audit logging function for integration with admin dashboard
def log_critic_analysis(analysis: CriticAnalysis, contract: Dict, admin_user: str = None, override_reason: str = None):
    """Log critic analysis for admin dashboard and audit trail"""
    
    log_entry = {
        'timestamp': analysis.analysis_timestamp.isoformat(),
        'contract_id': contract.get('id', 'unknown'),
        'contract_title': contract.get('title', 'N/A'),
        'rubric_version': analysis.rubric_version,
        'overall_score': analysis.overall_score,
        'market_balance_score': analysis.market_balance_score,
        'passed': analysis.passed,
        'blocked': analysis.blocked,
        'issues_found': len(analysis.issues_found),
        'blocking_issues': len(analysis.blocking_issues),
        'admin_override_required': analysis.admin_override_required,
        'adversarial_test_result': analysis.adversarial_test_result,
        'bookie_viability': analysis.bookie_viability_assessment,
        'rejection_reason': analysis.rejection_reason,
        'admin_user': admin_user,
        'override_reason': override_reason,
        'variant_analysis': analysis.variant_analysis
    }
    
    # Log to file and/or database
    logger.info(f"Contract Critic Analysis: {json.dumps(log_entry, indent=2)}")
    
    return log_entry
